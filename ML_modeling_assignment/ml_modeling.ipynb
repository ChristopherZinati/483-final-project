{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MAX_DATA_PATH = Path(\"C:/Users/maxpd/Desktop/483 ML/Intrusion Detection/483-final-project-datasets/OPCUA_dataset_public.csv\")\n",
    "CHRIS_DATA_PATH = Path(\"/Users/chriszinati/Desktop/CPSC-483/final project datasets/483-final-project-datasets/OPCUA_dataset_public.csv\")\n",
    "MAX_DATA_PATH_REDUCED_CLEANED = Path(\"C:/Users/maxpd/Desktop/483 ML/Intrusion Detection/483-final-project-datasets/cleaned_data_for_OCPUA.csv\")\n",
    "CHRIS_DATA_PATH_CLEANED = Path(\"/Users/chriszinati/Desktop/CPSC-483/final project datasets/483-final-project-datasets/cleaned_data_for_OCPUA.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**build 2 models of same algorithm using full and reduced feature set**\n",
    "we are using random forest here.\n",
    "1. import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create full feature and reduced datasets from csv (we already reduced features during preprocessing, so we will bring the original dataset and clean it but without dropping features (only dropping instances that aren't DDoS or regular traffic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features = pd.read_csv(CHRIS_DATA_PATH)\n",
    "preprocessed_reduced = pd.read_csv(CHRIS_DATA_PATH_CLEANED)\n",
    "\n",
    "full_features = full_features.dropna(axis=0) #drop rows with missing values\n",
    "instancesToDrop= ['MITM', 'Impersonation'] \n",
    "full_features = (full_features[full_features.multi_label.isin(instancesToDrop) == False]) #drop instances of MITM and Impersonation\n",
    "full_features = full_features.drop('multi_label', axis=1) #drop multiclass label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in ['proto', 'service', 'service_errors', 'status_errors', 'src_ip', 'dst_ip']:\n",
    "    if full_features[col].dtype == 'object' or full_features[col].dtype.name == 'category':\n",
    "        le = LabelEncoder()\n",
    "        full_features[col] = le.fit_transform(full_features[col])\n",
    "\n",
    "\n",
    "X_full = full_features.iloc[:, :-1]\n",
    "y_full = full_features.iloc[:, -1]\n",
    "\n",
    "X_reduced = preprocessed_reduced.iloc[:, :-1]\n",
    "y_reduced = preprocessed_reduced.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. train 2 random forest models on reduced and full-feature training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test = train_test_split(X_reduced, y_reduced, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model1.fit(X_reduced_train, y_reduced_train)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "filename='finalized_model_M1.sav'\n",
    "filename2='finalized_model_M2.sav'\n",
    "\n",
    "pickle.dump(model1, open(filename, 'wb'))\n",
    "pickle.dump(model2, open(filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load the saved models from disc and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_1 = pickle.load(open(filename, 'rb'))\n",
    "loaded_model_2 = pickle.load(open(filename2, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model_1.score(X_reduced_test, y_reduced_test)\n",
    "result2 = loaded_model_2.score(X_test, y_test)\n",
    "\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chriszinati\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
